{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c265767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import NearMiss, ClusterCentroids, RandomUnderSampler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# input the pre-processed train data\n",
    "df =  pd.read_csv('Train_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c9627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and Y\n",
    "Y = df.target\n",
    "X = df.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bae7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply random undersampling on X and Y\n",
    "rus = RandomUnderSampler(random_state = 0)\n",
    "X_rus,Y_rus = rus.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6985c",
   "metadata": {},
   "source": [
    "## RandomForest RandomizedSearchCV ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 250, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "#max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 15, num = 5)]\n",
    "print(max_depth)\n",
    "max_depth.append(None)\n",
    "#Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "#Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'bootstrap': bootstrap,\n",
    "               'min_samples_split':min_samples_split,\n",
    "               'min_samples_leaf':min_samples_leaf\n",
    "               }\n",
    "\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "clf = RandomizedSearchCV(rfc, random_grid, random_state=42, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "clf.fit(X_rus, Y_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff2fd9",
   "metadata": {},
   "source": [
    "Best Model of Random Forest is RandomForestClassifier(random_state=42, n_estimators = 100, min_samples_split = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0f937",
   "metadata": {},
   "source": [
    "## RandomizedSearch on Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1088b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from scipy.stats import uniform\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "clf = RandomizedSearchCV(logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "clf.fit(X_rus, Y_rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43740e9b",
   "metadata": {},
   "source": [
    "Best model of Logistic regression is LogisticRegression(C=3.6920981421859334)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e8052",
   "metadata": {},
   "source": [
    "## RandomizedSearch on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_dist = {\"max_depth\": [None, 2, 5, 10, 12, 15],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "clf = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "clf.fit(X_rus,Y_rus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c54c1",
   "metadata": {},
   "source": [
    "Best model of Decision Tree is DecisionTreeClassifier(max_depth=12, max_features=5, min_samples_leaf=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the pre-processed test data\n",
    "X_test = pd.read_csv('Test_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab474add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting probabilities of each class and writing the results to 'predictions.csv' -- Including qc_score\n",
    "test_pred = clf.best_estimator_.predict_proba(X_test)\n",
    "pd.DataFrame(test_pred[:,1], columns=['target']).to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the qc_score to test the model without qc_score present in the data.\n",
    "X_nq = X.drop('qc_score', axis = 1)\n",
    "X_test_nq = X_test.drop('qc_score', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7ae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting probabilities of each class and writing the results to 'predictions.csv' -- Excluding qc_score\n",
    "test_nq_pred = clf.best_estimator_.predict_proba(X_test_nq)\n",
    "pd.DataFrame(test_pred[:,1], columns=['target']).to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ae254",
   "metadata": {},
   "source": [
    "## Classification Report ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b47ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "print(\"Training Set\")\n",
    "# replace test_pred with test_pred_nq when testing without qc_score\n",
    "print(classification_report(Y,test_pred,digits=5))\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(tree, X, Y, name=\"Random Forest\")\n",
    "_ = display.ax_.set_title(\"2-class Precision-Recall curve\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
